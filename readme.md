# Transformers From Scratch


- `SelfAttention.py` Implementasi Self Attention Mechanism yang dipakai di Transformers
- `modules.py` Berisi layer-layer pada model

## Source
- Attention is All You Need Paper
- [Transformers from scratch by Umar Jamil](https://www.youtube.com/watch?v=ISNdQcPhsts&t=108s)
- [Pytorch Transformers by Aladdin Persson](https://www.youtube.com/watch?v=U0s0f995w14&t=1905s)
