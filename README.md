# Transformers From Scratch

Implementasi Transformer dari awal menggunakan PyTorch untuk tugas terjemahan bahasa (English-Indonesian).

## Struktur File

- `SelfAttention.py` Implementasi Self Attention Mechanism yang dipakai di Transformers
- `modules.py` Berisi layer-layer pada model (InputEmbedding, PositionalEncoding, LayerNormalization, FeedForwardBlock, MultiHeadAttention, ResidualConnection, ProjectionLayer)
- `encoder.py` Bagian encoder dari model
- `decoder.py` Bagian decoder dari model
- `transformers_model.py` Full version dari model

## Dataset


## Source
- Attention is All You Need Paper
- [Transformers from scratch by Umar Jamil](https://www.youtube.com/watch?v=ISNdQcPhsts&t=108s)
- [Pytorch Transformers by Aladdin Persson](https://www.youtube.com/watch?v=U0s0f995w14&t=1905s)
